{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1ADMjTDnOV_O5HzHUbjrAXD-dRP5zG9WK",
      "authorship_tag": "ABX9TyP06M2YLtEV9Lz4mjYp/05P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RemiArbache/NLP-Project/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB7ONmrO1Dcb"
      },
      "source": [
        "# Sentiment Analysis \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MrZpnQvDuKC"
      },
      "source": [
        "\r\n",
        "## Outline\r\n",
        "- [Imports du projet et des librairies](#0)\r\n",
        "- [Part 1:  Pré-traitement](#1)\r\n",
        "    - [1.1  Nettoyage et tokenization](#1.1)\r\n",
        "- [Part 2:  Création de la pipeline](#2)\r\n",
        "- [Part 3:  TF-IDF](#3)\r\n",
        "- [Part 4:  Construction du modèle](#4)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jMtnbz7HJn"
      },
      "source": [
        "<a name=\"0\"></a>\r\n",
        "## Imports du projet et des librairies "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwVHMaHYxDTT"
      },
      "source": [
        "Veuillez importer les fichiers du projet via le lien ci-dessous : \r\n",
        "\r\n",
        "https://drive.google.com/drive/folders/18Qkedfia8GFN4-GgpXKFN5-w6swJHgkb?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKVht86NwLlv",
        "outputId": "a105f78b-37f2-48f2-ebb3-c9355d753d6b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5_eC1ru1K4s"
      },
      "source": [
        "from time import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Import Regex to clean up tweets\r\n",
        "import re\r\n",
        "\r\n",
        "\r\n",
        "import nltk, string\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZcRhcLxYWRQ",
        "outputId": "ce77bf41-8836-468b-afe1-375f4722a716"
      },
      "source": [
        "try:\r\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "    print('Device:', tpu.master())\r\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "except:\r\n",
        "    strategy = tf.distribute.get_strategy()\r\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: grpc://10.27.239.242:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.27.239.242:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.27.239.242:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of replicas: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djJ6W_pBYreo"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNYefqe3w1Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "80a70b1a-8b8a-4346-e776-0402b85d849f"
      },
      "source": [
        "DF = pd.read_csv('gdrive/MyDrive/Twitter_Dataset/tweets.csv',encoding='latin',usecols=[0, 5], # to take only 2 useful column\r\n",
        "                 names=[\"label\",\"tweet\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-11bc4875b060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m DF = pd.read_csv('gdrive/MyDrive/Twitter_Dataset/tweets.csv',encoding='latin',usecols=[0, 5], # to take only 2 useful column\n\u001b[0;32m----> 2\u001b[0;31m                  names=[\"label\",\"tweet\"])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqB_3Dhxz_Yv"
      },
      "source": [
        "DF['label'].replace([4, 0],[1, 0], inplace=True) # Replace 0 and 4 by 0 and 1 to clarify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1JnADpayYoX"
      },
      "source": [
        "print(DF.head(25))\r\n",
        "len(DF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-r3wiq3Pqe"
      },
      "source": [
        "On peut voir que les tweets contiennent des mentions, des urls, etc. qui ne sont pas utiles au modèle de langage. Il faut donc nettoyer leur contenu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx45o7nz6X2C"
      },
      "source": [
        "<a name=\"1\"></a>\r\n",
        "## Pré-Traitement\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feom1QFT4Kq6"
      },
      "source": [
        "<a name=\"1.1\"></a>\r\n",
        "### Nettoyage et Tokenization\r\n",
        "J'utilise la fonction clean trouvée sur Kaggle pour nettoyer les tweets.\r\n",
        "Les contractions sont séparées, les charactères spéciaux sont supprimés, ainsi que les URLs, les mentions, les mots trops courts, et les stopwords. \r\n",
        "\r\n",
        "Les tweets sont aussi transformés en Tokens grâce à la librairie ```nltk```.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnzzklWW3Iq6",
        "outputId": "51675412-33ab-4510-a429-53a76b4f4c12"
      },
      "source": [
        "tokenizer = TweetTokenizer(strip_handles=True)\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "corpus = []"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A65-xor3uxu"
      },
      "source": [
        "def clean(tweet): \r\n",
        "            \r\n",
        "    # Contractions\r\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\r\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\r\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\r\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\r\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\r\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\r\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\r\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\r\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\r\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\r\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\r\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\r\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\r\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\r\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\r\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\r\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\r\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\r\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\r\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\r\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\r\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\r\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\r\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\r\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\r\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\r\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\r\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\r\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\r\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\r\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\r\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\r\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\r\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\r\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\r\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\r\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\r\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\r\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\r\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\r\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\r\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\r\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\r\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\r\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\r\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\r\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\r\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\r\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\r\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\r\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\r\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\r\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\r\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\r\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\r\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\r\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\r\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\r\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\r\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\r\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\r\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\r\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\r\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\r\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\r\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\r\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\r\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\r\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\r\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\r\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\r\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\r\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\r\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\r\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\r\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\r\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\r\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\r\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\r\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\r\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\r\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\r\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\r\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\r\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\r\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\r\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\r\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\r\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\r\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \r\n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \r\n",
        "    \r\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\r\n",
        "    tweet = re.sub(r\"yrs\", \"years\", tweet)\r\n",
        "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\r\n",
        "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\r\n",
        "    tweet = re.sub(r\"2day\", \"today\", tweet)\r\n",
        "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\r\n",
        "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\r\n",
        "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\r\n",
        "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\r\n",
        "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\r\n",
        "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\r\n",
        "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\r\n",
        "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\r\n",
        "    tweet = re.sub(r\"goood\", \"good\", tweet)\r\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\r\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\r\n",
        "    # Character entity references\r\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\r\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\r\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\r\n",
        "    # Typos, slang and informal abbreviations\r\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\r\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\r\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\r\n",
        "    # Urls\r\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\r\n",
        "    # Numbers\r\n",
        "    tweet = re.sub(r'[0-9]', '', tweet)\r\n",
        "    # Eliminating the mentions\r\n",
        "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\r\n",
        "    # Remove punctuation and special chars (keep '!')\r\n",
        "    for p in string.punctuation.replace('!', ''):\r\n",
        "        tweet = tweet.replace(p, '')\r\n",
        "        \r\n",
        "    # ... and ..\r\n",
        "    tweet = tweet.replace('...', ' ... ')\r\n",
        "    if '...' not in tweet:\r\n",
        "        tweet = tweet.replace('..', ' ... ')\r\n",
        "        \r\n",
        "    # Tokenize\r\n",
        "    tweet_words = tokenizer.tokenize(tweet)\r\n",
        "    \r\n",
        "    # Eliminating the word if its length is less than 3\r\n",
        "    tweet = [w for w in tweet_words if len(w)>2]\r\n",
        "    \r\n",
        "    # remove stopwords\r\n",
        "    tweet = [w.lower() for w in tweet if not w in stop_words]  \r\n",
        "    \r\n",
        "    corpus.append(tweet)\r\n",
        "    \r\n",
        "    # join back\r\n",
        "    tweet = ' '.join(tweet)\r\n",
        "        \r\n",
        "        \r\n",
        "    return tweet"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra5JF2wM47He"
      },
      "source": [
        "Les abbréviations seront remplacées par leur homologue complet grâce à ce dictionnaire d'abbréviations et à la fonction ```convert_abbrev_in_text``` associée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXOuCl754r9x"
      },
      "source": [
        "variable_name = \"\"\n",
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\", \n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\", \n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", \n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\", \n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "     \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\", \n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "def convert_abbrev_in_text(tweet):\n",
        "    t=[]\n",
        "    words=tweet.split()\n",
        "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
        "    return ' '.join(t) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK6QMnZt7dT2"
      },
      "source": [
        "La fonction suivante exécute les deux fonctions définie au-dessus sur un tweet donné :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh22WxpntD5K"
      },
      "source": [
        "def prepare_string(tweet):\r\n",
        "  tweet = clean(tweet)\r\n",
        "  tweet = convert_abbrev_in_text(tweet)\r\n",
        "  return tweet"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0pRIsdK9gck"
      },
      "source": [
        "Cette étape peut prendre quelques minutes, elle applique la fonction de nettoyage à tous les tweets du corpus de texte et supprime les lignes qui sont vides après le nettoyage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLRIsX875MuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f7fa93-1dfc-4e1a-dbe6-d94db21186e5"
      },
      "source": [
        "%%time\r\n",
        "# Apply prepare_string to all rows in 'tweets' column\r\n",
        "DF['tweet'] = DF['tweet'].apply(lambda s : prepare_string(s))\r\n",
        "\r\n",
        "# Drop empty values from dataframe\r\n",
        "DF['tweet'].replace('', np.nan, inplace=True)\r\n",
        "DF.dropna(subset=['tweet'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 59s, sys: 862 ms, total: 5min\n",
            "Wall time: 5min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Rqmx0YFJ5ee-",
        "outputId": "4449463b-035e-418f-951e-9341cc195548"
      },
      "source": [
        "DF.head(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>awww bummer shoulda got david carr third day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>upset cannot update facebook texting might cry...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>dived many times ball managed save rest bounds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>whole body feels itchy like fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>behaving mad cannot see</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0</td>\n",
              "      <td>going kill seen waiting till one solid week si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0</td>\n",
              "      <td>think right hahaa hours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0</td>\n",
              "      <td>hate see spartans sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0</td>\n",
              "      <td>mind body severely protesting quotgetting upqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0</td>\n",
              "      <td>goin follow since laughing loud angels</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>207 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                              tweet\n",
              "0        0       awww bummer shoulda got david carr third day\n",
              "1        0  upset cannot update facebook texting might cry...\n",
              "2        0     dived many times ball managed save rest bounds\n",
              "3        0                   whole body feels itchy like fire\n",
              "4        0                            behaving mad cannot see\n",
              "..     ...                                                ...\n",
              "203      0  going kill seen waiting till one solid week si...\n",
              "204      0                            think right hahaa hours\n",
              "205      0                              hate see spartans sad\n",
              "206      0  mind body severely protesting quotgetting upqu...\n",
              "207      0             goin follow since laughing loud angels\n",
              "\n",
              "[207 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU2nNYdOTr83"
      },
      "source": [
        "Le DataFrame obtenu est converti en CSV et téléchargé afin d'éviter la ré-exécution du code précédent qui est gourmande en ressources et en temps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_KI5RsToKz"
      },
      "source": [
        "DF.to_csv('gdrive/MyDrive/Twitter_Dataset/cleaned_tweets.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiP8lPpCDJmG"
      },
      "source": [
        "Les données sont maintenant prêtes à être soumises aux différentes méthodes de traitement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcqoN0GUr8Xx"
      },
      "source": [
        "<a name=\"2\"></a>\r\n",
        "## Création de la pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJvlQ72tWe4"
      },
      "source": [
        "Une pipeline gérant le traitement et l'analyse d'un tweet donné est construite.\r\n",
        "Elle prend en entrée un tweet et une fonction d'analyse et retourne la prédiction (positive ou négative) de l'analyseur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pihO7VaktjRP"
      },
      "source": [
        "def pipeline(tweet, analyze):\r\n",
        "  tweet = prepare_string(tweet)\r\n",
        "  return analyze(tweet)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4QtrcaIvu0v"
      },
      "source": [
        "<a name=\"3\"></a>\r\n",
        "## TF-IDF\r\n",
        "Utilisation de la TF-IDF pour entraîner un modèle capable de prédire le sentiment d'un tweet.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXq82OWD4O9C",
        "cellView": "form"
      },
      "source": [
        "#@title Chargement des données\n",
        "#@markdown Si le pré-traitement n'a pas été effectué, il est possible de charger les données pré-traitées depuis le Drive (il faut exécuter la cellule pour valider le choix).\n",
        "load_saved_data = True #@param {type:\"boolean\"}\n",
        "\n",
        "#Optionnel : Rechargement des tweets nettoyés à partir du drive \n",
        "if load_saved_data:\n",
        "  DF = pd.read_csv('gdrive/MyDrive/Twitter_Dataset/cleaned_tweets.csv',encoding='latin', names=[\"label\",\"tweet\"], header=None, skiprows=1)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J61ukuMjHyXs"
      },
      "source": [
        "# Imports\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \r\n",
        "from sklearn.model_selection import train_test_split  \r\n",
        "from scipy.sparse import csc_matrix\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1S6ll4V4eXR"
      },
      "source": [
        "# Utiliser toutes les données excède la capacité de RAM du Notebook \r\n",
        "corpus_size = int(20000)\r\n",
        "\r\n",
        "# Les tweets sont choisis au début et à la fin du jeu de données pour avoir des sentiments positifs et négatifs à parts égales\r\n",
        "tweets = [*DF['tweet'].values[:int(corpus_size/2)], *DF['tweet'].values[-int(corpus_size/2):]]\r\n",
        "# De même pour les targets assoicées\r\n",
        "y = [*DF['label'].values[:int(corpus_size/2)], *DF['label'].values[-int(corpus_size/2):]]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw7xeei840Cj"
      },
      "source": [
        "# J'utilise la bibliothèque sklearn qui implémente la TF-IDF\r\n",
        "\r\n",
        "tfIdfVectorizer = TfidfVectorizer()\r\n",
        "X = tfIdfVectorizer.fit_transform(tweets).toarray()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaedoUXI5d3j"
      },
      "source": [
        "# Le jeu de données est découpé pour entraîner le classifier\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfQfMCY3KGL5",
        "outputId": "6de217af-1681-4ef2-bb61-7a042e75350a"
      },
      "source": [
        "# La classification est une RandomForestClassifier dont le nombre d'estimateurs a été réduit à 15 pour des raisons de RAM\r\n",
        "text_classifier = RandomForestClassifier(n_estimators=15, random_state=0)  \r\n",
        "text_classifier.fit(X_train, y_train )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=15,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu7mzwUJ5jJ_",
        "outputId": "2802c3f0-0e56-4f27-8588-3e4a3f0266a1"
      },
      "source": [
        "predictions = text_classifier.predict(X_test)\r\n",
        "print(confusion_matrix(y_test,predictions))  \r\n",
        "print(classification_report(y_test,predictions))  \r\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1334  660]\n",
            " [ 444 1562]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.67      0.71      1994\n",
            "           1       0.70      0.78      0.74      2006\n",
            "\n",
            "    accuracy                           0.72      4000\n",
            "   macro avg       0.73      0.72      0.72      4000\n",
            "weighted avg       0.73      0.72      0.72      4000\n",
            "\n",
            "0.724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUru-YD4PUH6"
      },
      "source": [
        "\r\n",
        "On observe que le modèle a un taux correct de précision et évite relativement les faux positifs et les faux négatifs.\r\n",
        "\r\n",
        "---\r\n",
        "Le classifier et le vectorizer sont sauvegardés pour réduire le temps de calcul lors de l'utilisation normale.\r\n",
        "Le vectorizer contient le vocabulaire appris et le classifier permet d'estimer le sentiment du tweet.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WjoXEzJ35q8",
        "outputId": "b3de7da2-6ed0-446e-ecb7-cd4c07dcf468"
      },
      "source": [
        "from joblib import dump, load\r\n",
        "dump(text_classifier, 'gdrive/MyDrive/Twitter_Dataset/RandomForestTextClassifier.joblib')\r\n",
        "dump(tfIdfVectorizer, 'gdrive/MyDrive/Twitter_Dataset/tfIdfVectorizer.joblib')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gdrive/MyDrive/Twitter_Dataset/tfIdfVectorizer.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJeZi8kSZQJf",
        "outputId": "05ad9fbf-d636-4779-cf88-41c3b7893d95"
      },
      "source": [
        "v = 15\r\n",
        "inv_map = {v: k for k, v in tfIdfVectorizer.vocabulary_.items()}\r\n",
        "words = [inv_map.get(i) for i,c in enumerate(X_test[v]) if c != 0]\r\n",
        "print(words)\r\n",
        "print(X_test[v])\r\n",
        "print()\r\n",
        "text_classifier.predict(X_test[v].reshape(1,-1))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itunes', 'liking', 'mean', 'new', 'one', 'pricing', 'seen', 'several', 'songs']\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5bQ3-6YRZ9r"
      },
      "source": [
        "### Pipeline complète d'utilisation de la TF-IDF :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihS3g5wgAavo"
      },
      "source": [
        "import httplib2\r\n",
        "import requests\r\n",
        "import urllib3\r\n",
        "import sys\r\n",
        "sys.path.insert(0,'/content/drive/My Drive/Twitter_Dataset')\r\n",
        "import random_tweets as rt\r\n",
        "\r\n",
        "loaded_classifier = load('gdrive/MyDrive/Twitter_Dataset/RandomForestTextClassifier.joblib')\r\n",
        "loaded_vectorizer = load('gdrive/MyDrive/Twitter_Dataset/tfIdfVectorizer.joblib')\r\n",
        "\r\n",
        "# Your Twitter API credentials here : \r\n",
        "credentials = {\r\n",
        "        'consumer_key': '',\r\n",
        "        'consumer_secret': ''\r\n",
        "    }\r\n",
        "\r\n",
        "def tf_idf_pipeline(subject :str, credentials):\r\n",
        "  raw_tweet = rt.get_random_tweet(subject, credentials)['text']\r\n",
        "  print(raw_tweet)\r\n",
        "  cleaned_tweet = prepare_string(raw_tweet)\r\n",
        "  print(cleaned_tweet)\r\n",
        "  word_vector = loaded_vectorizer.transform([cleaned_tweet])\r\n",
        "\r\n",
        "  return bool(loaded_classifier.predict(word_vector.reshape(1,-1))[0])\r\n",
        "\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6kJuVNfFQ_z",
        "outputId": "a3c0cd01-fa92-4286-efc0-9b780d321db8"
      },
      "source": [
        "tf_idf_pipeline(\"pizza\", credentials)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@tiziamalavita Pizza geht immer, ist doch quasi Brot 🤔\n",
            "pizza geht immer ist doch quasi brot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DHwrWbDG1b"
      },
      "source": [
        "<a name=\"4\"></a>\r\n",
        "## RNN & LSTM : GLoVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6RVQNKP-3ce"
      },
      "source": [
        "#@title Chargement des données\n",
        "#@markdown Si le pré-traitement n'a pas été effectué, il est possible de charger les données pré-traitées depuis le Drive.\n",
        "load_saved_data = True #@param {type:\"boolean\"}\n",
        "\n",
        "#Optionnel : Rechargement des tweets nettoyés à partir du drive:\n",
        "if load_saved_data:\n",
        "  DF = pd.read_csv('gdrive/MyDrive/Twitter_Dataset/cleaned_tweets.csv',encoding='latin', names=[\"label\",\"tweet\"], header=None, skiprows=1)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpqj6CAfDqRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bff1fd-d5e1-4d38-f9e5-6d6f18fc08ba"
      },
      "source": [
        "corpus = list(zip(DF['tweet'].values, DF['label'].values))\r\n",
        "\r\n",
        "\r\n",
        "embedding_dim = 100    # Dimensions utilisées pour glove6b100\r\n",
        "max_length = 20        # Taille maximale d'un tweet\r\n",
        "trunc_type='post'      # Tronque le tweet s'il est plus long que max_length\r\n",
        "padding_type='post'    # Ajoute du padding à la fin du tweet s'il est plus court que max_length\r\n",
        "oov_tok = \"<OOV>\"      # Token \"<OOV>\" remplace les mots qui ne font pas partie du vocabulaire (Out Of Vocabulary)\r\n",
        "training_size=len(corpus)\r\n",
        "test_portion=.025\r\n",
        "\r\n",
        "\r\n",
        "corpus[:5]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('awww bummer shoulda got david carr third day', 0),\n",
              " ('upset cannot update facebook texting might cry result school today also blah',\n",
              "  0),\n",
              " ('dived many times ball managed save rest bounds', 0),\n",
              " ('whole body feels itchy like fire', 0),\n",
              " ('behaving mad cannot see', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS6Rj6rBGJyv"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import random "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt29Cm8IGrSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faeca032-09a9-4c47-dc6b-cc9ac5b72c41"
      },
      "source": [
        "sentences=[]\r\n",
        "labels=[]\r\n",
        "\r\n",
        "random.shuffle(corpus)\r\n",
        "\r\n",
        "for x in range(training_size):\r\n",
        "    sentences.append(str(corpus[x][0]))\r\n",
        "    labels.append(corpus[x][1])\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "\r\n",
        "word_index = tokenizer.word_index\r\n",
        "vocab_size=len(word_index)\r\n",
        "\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n",
        "\r\n",
        "split = int(test_portion * training_size)\r\n",
        "\r\n",
        "test_sequences = padded[0:split]\r\n",
        "training_sequences = padded[split:training_size]\r\n",
        "test_labels = labels[0:split]\r\n",
        "training_labels = labels[split:training_size]\r\n",
        "print(sentences[split + 10])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whatever girl even like work thing promise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiZwqg0aJAer",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "c7a2cbf5-f531-497d-d4fe-84e87358d59d"
      },
      "source": [
        "embeddings_index = {};\r\n",
        "\r\n",
        "\r\n",
        "with open('gdrive/MyDrive/Twitter_Dataset/glove.6B.100d.txt') as f:\r\n",
        "    for line in f:\r\n",
        "        values = line.split();\r\n",
        "        word = values[0];\r\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\r\n",
        "        embeddings_index[word] = coefs;\r\n",
        "\r\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word);\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embeddings_matrix[i] = embedding_vector;"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-559444352941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0membeddings_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc4iTiqSX94C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "9f41f998-39e7-4a6d-c5d2-a1456cd04287"
      },
      "source": [
        "with strategy.scope():    \r\n",
        "    \r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\r\n",
        "        tf.keras.layers.Dropout(0.5),\r\n",
        "        tf.keras.layers.Bidirectional(LSTM(units=64, return_sequences=True)),\r\n",
        "        tf.keras.layers.Bidirectional(LSTM(units=128)),\r\n",
        "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\r\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "    ])\r\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "       \r\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-45522ea83e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     model = tf.keras.Sequential([\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATEe5yMhoppi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "6d145c24-cdaf-4134-8935-e44d2b2f43b7"
      },
      "source": [
        "    num_epochs = 15\r\n",
        "    \r\n",
        "    training_padded = np.array(training_sequences)\r\n",
        "    training_labels = np.array(training_labels)\r\n",
        "    testing_padded = np.array(test_sequences)\r\n",
        "    testing_labels = np.array(test_labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a5c322c85c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtesting_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_sequences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh75HkKJolB_"
      },
      "source": [
        "  history = model.fit(training_padded, \r\n",
        "                        training_labels, \r\n",
        "                        epochs=num_epochs, \r\n",
        "                        validation_data=(testing_padded, testing_labels),\r\n",
        "                        batch_size = 256,\r\n",
        "                        verbose=1)\r\n",
        "    \r\n",
        "    print(\"Training Complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdBcaDrT21hf"
      },
      "source": [
        "model.save('gdrive/MyDrive/Twitter_Dataset/trainedModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QICOugHYQWq"
      },
      "source": [
        "import matplotlib.image  as mpimg\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "#-----------------------------------------------------------\r\n",
        "# Retrieve a list of list results on training and test data\r\n",
        "# sets for each training epoch\r\n",
        "#-----------------------------------------------------------\r\n",
        "acc=history.history['accuracy']\r\n",
        "val_acc=history.history['val_accuracy']\r\n",
        "loss=history.history['loss']\r\n",
        "val_loss=history.history['val_loss']\r\n",
        "\r\n",
        "epochs=range(len(acc)) # Get number of epochs\r\n",
        "\r\n",
        "#------------------------------------------------\r\n",
        "# Plot training and validation accuracy per epoch\r\n",
        "#------------------------------------------------\r\n",
        "plt.plot(epochs, acc, 'r')\r\n",
        "plt.plot(epochs, val_acc, 'b')\r\n",
        "plt.title('Training and validation accuracy')\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "#------------------------------------------------\r\n",
        "# Plot training and validation loss per epoch\r\n",
        "#------------------------------------------------\r\n",
        "plt.plot(epochs, loss, 'r')\r\n",
        "plt.plot(epochs, val_loss, 'b')\r\n",
        "plt.title('Training and validation loss')\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\r\n",
        "\r\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVqvncQUoPh5"
      },
      "source": [
        "from tensorflow import keras\r\n",
        "model = keras.models.load_model('gdrive/MyDrive/Twitter_Dataset/trainedModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDh5KsXa5F_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "8b56e917-58b5-4189-82bf-8ed4fa3d6161"
      },
      "source": [
        "prediction = model.predict(training_padded[10].reshape(1,-1))\r\n",
        "\r\n",
        "print(prediction, training_labels[10], training_padded[10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ea2946e6baff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_padded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22vDgio_5qdQ"
      },
      "source": [
        "training_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_L6GQD5Lqhq"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUEjFOl-7zMl"
      },
      "source": [
        "## Random Tweet\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOIHRkwU72Kg"
      },
      "source": [
        "import httplib2\r\n",
        "import requests\r\n",
        "import urllib3\r\n",
        "\r\n",
        "!cp /content/drive/MyDrive/Twitter_Dataset/random_tweets.py .\r\n",
        "import random_tweets as rt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w-KHvNAt_4NW",
        "outputId": "009af4ab-22a1-40c0-b63a-cab1a42a5559"
      },
      "source": [
        "credentials = {\r\n",
        "        'consumer_key': 'UNLbXFERhngQ8AEOJ0a03BSpn',\r\n",
        "        'consumer_secret': '7CausNjICjdjjiraYLR7vYIica7sUZE6UsS6eRHVGU6B3siQ7k'\r\n",
        "    }\r\n",
        "rt.get_random_tweet(\"processing\", credentials)['text']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Supporting humanitarian scientific research is important to me. That's why I donated 18 days of my computer's proce… https://t.co/ohhWPPIUJ8\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE0hfHa9EtVd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}